<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Mamba Review</title>
    <link rel="icon" href="favicon.png" type="image/png">

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/simple-blog-template.css" rel="stylesheet">

    <!-- Marked.js for Markdown parsing -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

    <!-- MathJax for LaTeX rendering -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        /* 수식을 포함한 모든 내용을 왼쪽 정렬 */
        .content-container {
            text-align: left;
        }

        /* LaTeX 수식을 왼쪽 정렬 */
        .MathJax_Display {
            text-align: left !important;
        }
    </style>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="index.html"></a>
            </div>
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="index.html">About</a></li>
                    <li><a href="blog.html" style="color: #06B968;">Blog</a></li>
                    <li><a href="gitrepo.html">Sign up</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Page Content -->
    <div class="container content-container">
        <div class="row">
            <div class="col-lg-12">
                <h1 class="post-title">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h1>
                <hr>
                <p><span class="glyphicon glyphicon-time"></span> Updated 2024-08-16 </p>
                <hr>

                <!-- Markdown Content -->
                <div id="markdown-content">
                    <!-- 이곳에 변환된 Markdown 내용이 삽입됩니다 -->
                </div>

                <hr>

                <!-- Comments Section -->
                <div class="well">
                    <h4>Leave a Comment:</h4>
                    <form role="form">
                        <div class="form-group">
                            <textarea class="form-control" rows="3"></textarea>
                        </div>
                        <button type="submit" class="btn btn-primary">Submit</button>
                    </form>
                </div>

                <hr>

                <!-- Posted Comments -->
                <div class="media">
                    <a class="pull-left" href="#">
                        <img class="media-object" src="http://placehold.it/64x64" alt="">
                    </a>
                    <div class="media-body">
                        <h4 class="media-heading">Start Bootstrap
                            <small>August 25, 2014 at 9:30 PM</small>
                        </h4>
                        Cras sit amet nibh libero, in gravida nulla. Nulla vel metus scelerisque ante sollicitudin commodo. Cras purus odio, vestibulum in vulputate at, tempus viverra turpis. Fusce condimentum nunc ac nisi vulputate fringilla. Donec lacinia congue felis in faucibus.
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <p>Copyright &copy; Your Website 2014</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>
    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Custom Script to Convert Markdown and Insert into HTML -->
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            const markdownContent = `

[HiPPO: Recurrent Memory with Optimal Polynomial Projections](https://arxiv.org/abs/2008.07669)

[Combining Recurrent, Convolutional, and Continuous-time Models...](https://arxiv.org/abs/2110.13985)

[Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396)

[The Annotated S4](https://srush.github.io/annotated-s4/)

[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)

[Transformers are SSMs: Generalized Models and Efficient Algorithms...](https://arxiv.org/abs/2405.21060)

# Background(주로 HiPPO에 대한)

### #. HiPPO의 Online Function Approximation

- **무한 차원**의 함수를 컴퓨터의 제한된 **유한 차원**으로 점진적으로 근사화 해야함
- 매 Time Step마다 함수에 대해서 잘 근사화가 되었는지 확인해야함
- 매 Time Step마다 **적절한 basis**들을 사용해서 **함수를 근사화**하려고 함

<img src="mamba_review_img/Untitled.png" alt="HiPPO-1" width="1000">

- ***적절한  Basis***
    
    **Orthogonal Basis**
    
    - HiPPO에서 말하는 적절한 Basis는 Orthogonal basis이다. 아래에 간단한 예시를 들어서 설명하면
        
        <img src="mamba_review_img/Untitled%201.png" alt="Orthogonal Basis" width="900">
        

- ***잘 근사화가 되었는가?***
    
    ⇒ 수식적으로 원래 함수와 근사화된 함수 사이의 **Distance(거리)를 정의**해야함
    
    <img src="mamba_review_img/f7797c9f-6b15-4512-9e1b-4ea0d74397c9.png" alt="distance" width="900">
    

---

### #. Lebesgue Integral

<img src="mamba_review_img/Untitled%202.png" alt="Lebesgue" width="1000">

하지만, 이런 문제를 ***Lebegue Integral***로 해결할 수 있다.

<img src="mamba_review_img/Untitled%203.png" alt="Lebegue Integral" width="1000">

1. 기존의 Riemann 적분처럼 $x$축으로 구간을 나누는게 아닌 $y$축으로 구간을 나눈 후 
2. 각 $y$축 값에 해당하는 $x$축 값의 집합을 measure $\mu$로 정의해서 적분을 한다.

<img src="mamba_review_img/Untitled%204.png" alt="Lebegue Integral-1" width="1000">

⇒ HiPPO논문에서는 time-domain에서의 Measure를 $\mu^{(t)}$로 표현하고 이는 시간축에 따라서 $\mu^{(t)}$ 값이 클수록 Lebesgue Integral에서 중요한 값으로 해석가능하다.

⇒ 더 단순하게 생각하면 measure $\mu^{(t)}$는 중요한 구간에 대한 가중치를 부여한다고 해석이 가능하다.

[Wikiwand - Lebesgue integral](https://www.wikiwand.com/en/Lebesgue_integration#/Via_improper_Riemann_integral)

---

### # Densities

<img src="mamba_review_img/Untitled%205.png" width="1000">

---

### State Space Model

<img src="mamba_review_img/Untitled%206.png" width="1000">

제어공학에서 다루는 SSM

- $$\\dot{x}(t) = \\mathbf{A}x(t) + \\mathbf{B}u(t)$$
- $$y(t) = \\mathbf{C}x(t) + \\mathbf{D}u(t)$$
    - input $$u(t) \\in \\mathbb{R}^{1\\times L}$$
    - state respresentation $$x(t) \\in \\mathbb{R}^{N \times L}$$
    - output $$y(t) \\in \\mathbb{R}^{1\\times L}$$

주된 특성으로는 **LTI System**이라는 점이다.

In Sequence-to-Sequence Mapping

- $\dot{x}(t) = \mathbf{A}x(t) + \mathbf{B}u(t)$ ⇒ Encoder로 해석 가능
- $y(t) = \mathbf{C}x(t) + \mathbf{D}u(t)$ ⇒ Decoder로 해석 가능

<img src="mamba_review_img/Untitled%207.png" width="1000">

### # ODE Solver (Neural ODEs)

![Untitled](mamba_review_img/Untitled%208.png)

***In Residual Network***

![Untitled](mamba_review_img/Untitled%209.png)

$x_{k+1} = x_k + f(x_k)$

***In ODE Network***

![Untitled](mamba_review_img/Untitled%2010.png)

$\dfrac{d}{dt}x = f(x)$

 $x(k) = x(k_0) + \displaystyle \int_{k_0}^k f(s, x(s))ds$

[proceedings.neurips.cc](https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf)

**Neural Ordinary Differential Equations(Neural ODEs)**는 ****기존 Deep Learning Network를 ODE의 Initial Value Problem(IVP)로 정의하고 이를 ODE solver로 해를 구하려고 시도했다.

이를 통한 여러 장점이 있지만, 주로 HiPPO, LSSL에서 다루는 장점으로는 다음과 같다.

- Sequence Data를 연속적으로 처리 가능 (**이론상으로 무한히 긴 Sequence Data를 모델링 가능**)
- **ODE solver**로 매 시점마다 Adoptive **Time Step $\Delta t$를 자동으로 조정가능**(Training X)
- Discrete Data를 다루더라도 Network 안에서는 각 시점들간의 장기종속성(Long-Term Dependencies)을 동적으로 포착가능

### # Legendre Polynomials

- **Legendre differential equation**
    
    : $(1-x^2)y^{\prime\prime}-2xy^\prime + n(n+1)y = 0$
    
    의 해가 **Legendre polynomial : $P_n(x)$** 이다.
    
    **서로 다른 $P_n(x)$들은 $-1 \leq x\leq1$에서 직교성을 가진다.**
    
    - $\int_{-1}^{1}P_n(x) P_m(x) dx = \dfrac{2}{2n+1}\delta_{nm}$
        - $\delta_{nm} = \begin{cases} 1 \;\;\; n = m\\ 0 \;\;\; n \neq m\end{cases}$
    - $P_n(x)$ 전개 (공학수학 복습)
        
        $y^{\prime\prime}-\dfrac{2x}{(1-x^2)}y^\prime + \dfrac{n(n+1)}{(1-x^2)}y = 0$
        
        $(1-x^2)$로 나누면 $x = -1, 1$ 은 Singular points, 수렴반경 : $|x| < 1$
        
        $y = \sum \limits_{r=0}^{\infin}a_rx^r$
        
        $y^\prime = \sum \limits_{r=1}^{\infin}ra_rx^{r-1}$
        
        $y^{\prime\prime} = \sum\limits_{r=2}^\infin r(r-1)a_rx^{r-2}$
        
        을 대입하면
        
        $(1-x^2)\sum\limits_{r=2}^\infin r(r-1)a_r x^{r-2} - 2x\sum \limits_{r=1}^{\infin}ra_rx^{r-1} + n(n+1)\sum \limits_{r=0}^{\infin}a_rx^r = 0$
        
        각 항을 $x^n$에 맞춰준다.
        
        $\begin{aligned}&\text{첫번째 항 : }(1-x^2)\sum\limits_{r=2}^\infin r(r-1)a_r x^{r-2}\\ &= \sum\limits_{r=2}^\infin r(r-1)a_r x^{r-2} -x^2\sum\limits_{r=2}^\infin r(r-1)a_r x^{r-2} \\&=\sum\limits_{k=0}^\infin (k+2)(k+1)a_{k+2}\, x^{k} -\sum\limits_{r=2}^\infin r(r-1)a_r x^{r}\;\;\;\;\; (\because k = r-2) \\ &\Rightarrow \sum\limits_{r=0}^\infin (r+2)(r+1)a_{r+2} \,x^{r}-\sum\limits_{r=2}^\infin r(r-1)a_r x^{r} \;\;\;\;\; (\because r = k, \text{notation만 다르고 연산은 똑같음})\end{aligned}$
        
        $\begin{aligned}&\text{두번째 항 : }-2x\sum\limits_{r=1}^\infin ra_r x^{r-1}\\ &=  -2\sum\limits_{r=1}^\infin ra_r x^{r}\end{aligned}$
        
        $\text{세번째 항 : }n(n+1)\sum\limits_{r=0}^\infin a_r x^{r}$
        
        $(1-x^2)\sum\limits_{r=0}^\infin (r+2)(r+1)a_{r+2} \,x^{r}-\sum\limits_{r=2}^\infin r(r-1)a_r x^{r} -2\sum\limits_{r=1}^\infin ra_r x^{r} + n(n+1)\sum\limits_{r=0}^\infin a_r x^{r} = 0$
        
        $x^n$으로 묶어야하는데 각 $\sum$의 시작 지수가 다름으로 $n=2$ 에 맞춘다.
        
        $\sum\limits_{r=0}^\infin (r+2)(r+1)a_{r+2} \,x^{r}-\sum\limits_{r=2}^\infin r(r-1)a_r x^{r} -2\sum\limits_{r=1}^\infin ra_r x^{r} + n(n+1)\sum\limits_{r=0}^\infin a_r x^{r} = 0$
        
        $2a_2 + 6a_3x + \sum\limits_{r=2}^\infin (r+2)(r+1)a_{r+2} \,x^{r}$$-\sum\limits_{r=2}^\infin r(r-1)a_r x^{r} -2\sum\limits_{r=1}^\infin ra_r x^{r} + n(n+1)\sum\limits_{r=0}^\infin a_r x^{r} = 0$
        
        $2a_2 + 6a_3x + \sum\limits_{r=2}^\infin (r+2)(r+1)a_{r+2} \,x^{r}-\sum\limits_{r=2}^\infin r(r-1)a_r x^{r}$ $-2\bigg(a_1 x + \sum\limits_{r=2}^\infin ra_r x^{r}\bigg)$$+ n(n+1)\sum\limits_{r=0}^\infin a_r x^{r} = 0$
        
        $2a_2 + 6a_3x + \sum\limits_{r=2}^\infin (r+2)(r+1)a_{r+2} \,x^{r}-\sum\limits_{r=2}^\infin r(r-1)a_r x^{r} -2\bigg(a_1 x + \sum\limits_{r=2}^\infin ra_r x^{r}\bigg)$$+ n(n+1)\bigg( a_0 + a_1x+\sum\limits_{r=2}^\infin a_r x^{r} \bigg)$$= 0$
        
        $2a_2 + n(n+1)a_0$  $+ \big\{ 6a_3 -2a_1 + n(n+1)a_1 \big\} x$  $+ \sum\limits_{r=2} ^\infin \bigg\{ (r+2)(r+1)a_{r+2} -r(r-1)a_r -2ra_r + n(n+1) a_r  \bigg\}x^r$$= 0$
        
        $\begin{cases}2a_2 + n(n+1)a_0 =0\\  6a_3 -2a_1 + n(n+1)a_1  =0 \\ (r+2)(r+1)a_{r+2} -r(r-1)a_r -2ra_r + n(n+1) a_r =0 \end{cases}$
        
        가 되야한다. 이를 정리하면 아래와 같이 된다.
        
        $\begin{cases}a_2 = -\dfrac{n(n+1)}{2}a_0 \\ \\ a_3 = \dfrac{(1-n)(n+2)}{6}a_1 \\ \\a_{r+2}=  \dfrac{r(r-1) +2r - n(n+1)}{(r+2)(r+1)} a_r = \dfrac{r(r+1) - n(n+1)}{(r+2)(r+1)} a_r  = \dfrac{(r-n)(r+n+1)}{(r+2)(r+1)} a_r\end{cases}$
        
        > $\therefore a_{r+2}= \dfrac{(r-n)(r+n+1)}{(r+2)(r+1)} a_r$
        > 
        
        - 이후 $P_n(x)$를 구하는 과정은 (1) Power Series 혹은 (2) Rodrigues’ formula를 사용하는 등 여러 방법이 있다.
        - (1) Power series
        1. 초기 조건 
            
            $P_0(x) = 1, P_1(x) = x$
            
        2. 전개 
            
            $y= \sum\limits_{r=0}^\infin a_rx^r = a_0 +a_1x + a_2x^2 + \cdots$ 에 대입
            
            $\mathbf{i})\; a_0 = 1, a_1 = 0$
            
            - $y_{even}(x) = 1  - \dfrac{n(n+1)}{2!}x^2 + \dfrac{(n-2)n(n+1)(n+3)}{4!}x^4 - \dfrac{(n-4)(n-2)n(n+1)(n+3)(n+5)}{6!}x^6 + \cdots$
            
            $\mathbf{ii})\; a_0 = 0, a_1 = 1$
            
            - $y_{odd}(x) = x  - \dfrac{(n-1)(n+2)}{3!}x^3 + \dfrac{(n-3)(n-1)(n+2)(n+4)}{5!}x^5 - \dfrac{(n-5)(n-3)(n-1)(n+2)(n+4)(n+6)}{7!}x^7 + \cdots$
            
            ![Untitled](mamba_review_img/bcb7dc0b-676a-42f4-a285-923400a77cfb.png)
            
            $n$의 even이면 $y_{even}(x)$가 polynomial이 되고 $y_{odd}(x)$가 series가 된다.
            
            $n$의 odd이면 $y_{odd}(x)$가 polynomial이 되고 $y_{even}(x)$ series가 된다.
            
            general solution꼴로 y_{even}(x)과 y_{odd}(x)의 선형 결합으로 표현하면 다음과 같다.
            
            > $y = c_1y_{even}(x) + c_2y_{odd}(x)$
            > 
            
            이 때, polynomial solution이 $P_n(1) = 1$ 의 성질(normalization 과정)을 가지도록 만들면
            
            > $P_n(x) = \dfrac{p_n(x)}{p_n(1)}=\begin{cases} \dfrac{y_{even}(x)}{y_{even}(1)} , \;\;\text{if}\; n = \text{even}\\ \dfrac{y_{odd}(x)}{y_{odd}(1)} , \;\;\text{if}\; n = \text{odd}\end{cases}$
            > 
            
            이렇게 최종적으로 Legendre Polynomial $P_n(x)$ 을 구할 수 있다.
            
            [르장드르 다항식(Legendre Polynomials)](https://blog.naver.com/qio910/222128469347)
            
        
    - $P_n(x)$ 성질
        
        [르장드르 다항식](https://ko.wikipedia.org/wiki/르장드르_다항식)
        
        ![Untitled](mamba_review_img/Untitled%2011.png)
        

---

# Introduction

- 같은 문장에 대한 Speech Data(Continuous Signal)와 Text data와 비교할 때, Speech Data가 처리할 데이터가 훨씬 많은 것처럼

![unnamed.gif](mamba_review_img/unnamed.gif)

[WaveNet: A generative model for raw audio](https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/)

- Continuous Signal Data에 대한 Long Range Dependencies를 처리하는 것은 Text Data뿐만이 아니라 Continuous Signal Data에서도 중요했다.
- 하지만, 기존에 있던 Fixed Length Vector에 Entire Context를 담는 방식(RNNs)은 한계가 너무 명확했고 Fixed Length Vector를 사용하지 않고 Global Feature를 분석할 수 있는 Transformer model은 End-to-End 방식을 적용하는데에 한계가 있었다.
1. Deep Learning에서 주로 사용되는 Online Learning으로 End-to-End 방식으로 최적의 방식(Orthogonal Basis를 사용해서)으로 Entrie Context를 근사화하는 초기화 방식 : **HiPPO(High-Order Polynomial Projection Operators)**
2. 이후 HiPPO는 RNNs에서 단순 Memory Unit 그쳤기 때문에 기존 RNNs가 병렬 학습이 안되어 학습시간이 느리다는 단점이 있었다. 이때, HiPPO의 SSM(State Space Model)을 Linear RNNs으로 해석해서 Convolution 연산을 도입해 병렬 학습을 진행할 수 있게 되었다. 또한 Inference시에는 기존의 Recurrent한 빙식으로 효율적인 Inference로 바꿀 수 있게 된 방식 : **LSSL(The Linear State Space Layer)**
3. 하지만 HiPPO → LSSL에서 사용된 SSM을 활용하기 위한 과정 중 Discrete하는 과정과 Convolutional Kernel을 만드는 과정이 있다. 이때, 들어가는 변수인 Step Size $\Delta t$와 Matrix $A$를 구하는 과정에서 Complexity가 높기 때문에, 더 효율적인 방식으로써 DPLR(**Diagonal Plus Low-Rank**)을 적용해 게산 효율성을 높인 방식 : **S4(Structured State Space for Sequence Modeling)**
4. S4 model을 NLP domain에 적용하기 위해서 부족한 방식을 적용한 Model : Mamba
5. Mamba의 부족한 표현력을 Attention 연산을 적용해서 부족한 표현력을 보완한 Model : Mamba-2

# HiPPO Framework(General Form)

![Untitled](mamba_review_img/Untitled%2012.png)

![Untitled](mamba_review_img/Untitled%2013.png)

### HiPPO의 1차 목적 :

Step 1. $t$ 시점까지의 축적된 함수 $f_{x \leq t}$를 주어진 measure $\mu^{(t)}$에 대해서 가장 잘 근사화(오차가 적은)하는 최적의 근사화된 함수 $g^{(t)}$ 를 구하는 것

![Untitled](mamba_review_img/Untitled%2014.png)

Step 2. 이때의 $g^{(t)}$의 coefficient $c_n(t)$를 구하는 것 ($c(t) := \text{memory vector}$)

![Untitled](mamba_review_img/Untitled%2015.png)

### $\therefore$  $g^{(t)}$를 구성하는 $\text{coefficient }c_n(t)$ 와 $\text{orthogonal basis }g_n^{(t)}$를 구해야한다.

> (normalized measure $\nu^{(t)}$)에 대한 $\text{orthogonal basis}$ $g_n^{(t)}$
> 
> 
> ## $g_n^{(t)} = \lambda_n \zeta(t)^{\frac{1}{2}}p_n^{(t)}\chi^{(t)}, \;\; n\in\mathbb{N}$
> 

> $\text{coefficient}$ $c_n(t)$
> 
> 
> ## $c_n(t) = \zeta(t)^{-\frac{1}{2}}\lambda_n \displaystyle\int fp_n^{(t)}\dfrac{\omega^{(t)}}{\chi^{(t)}}$
> 

## $\text{orthogonal basis}$ $g_n^{(t)}$ Expansion

 $\zeta(t)$ :  tilting까지 고려된 normalization constant(시간에 따라 변하지 않는 상수로 가정, 보통은 함수, **LegS에서는 constant로써 1**의 값을 가진다.)

> $\zeta (t) = \displaystyle \int \dfrac{\omega}{\chi^2} = \displaystyle \int \dfrac{\omega^{(t)}(x)}{(\chi^{(t)}(x))^2}\text{d}x$
> 
- $\chi$ : OPs를 사용하지 않는 경우(Laguerre, Chebyshev)에 대해 수식화하기 위해 만든 tilting function
    - $\chi$를 적용해서 tilting된 예시
    
    ![Untitled](mamba_review_img/Untitled%2016.png)
    
    1. $p_n(x)$ : 기존 Orthogonal Polynomials(OPs)
    2. $\bar{p}_n(x) = p_n(x) \chi(x)$ : 기존 OPs에서 tilting function이 곱해져서 나온 tilted fuction(가정)
    
    이 때, $\bar{p}_n(x)$는 $\dfrac{\omega}{\chi^2}$에 대해서 orthogonal하다.
    
    - Detail
        
        HiPPO는 $\text{Section }2.1$에서 설정한 inner product로 
        
        $\langle f, g \rangle_\mu = \int _0 ^\infin f(x)g(x) \;\text{d} \mu (x)$를 사용하기 때문에
        
        $\langle \bar{p}_n(x), \bar{p}_m(x) \rangle_\mu = \displaystyle\int _0 ^\infin \bar{p}_n(x)\bar{p}_m(x) \omega (x) \;\text{d} x =0$ (orthogonal)
        
        - $\omega$ ⇒ Scaling ⇒ $\dfrac{\omega}{\chi^2}$
        
        $\begin{aligned}&\langle \bar{p}_n(x), \bar{p}_m(x) \rangle_{\mu} \\ &= \displaystyle \int _0 ^\infin \bar{p}_n(x)\bar{p}_m(x)  \;\text{d}\mu\\ 
        &= \displaystyle \int _0 ^\infin p_n(x)\chi(x)p_m(x)\chi(x)  \;\text{d}\mu\\ 
        &= \displaystyle \int _0 ^\infin p_n(x)p_m(x)\chi^2(x)  \dfrac{\omega(x)}{\chi^2(x)}\;\text{d}x\\ 
        &=\displaystyle \int _0 ^\infin p_n(x)p_m(x)\omega(x)  \;\text{d}x=0 \end{aligned}$
        
        (여러 표현들이 생략됨, 그저 개념 이해를 위해 단순하게 전개)
        

> normalized measure $\nu^{(t)}$, normalized density $\omega^{(t)}\rightarrow$ $\dfrac{\omega^{(t)}}{\zeta(t)(\chi^{(t)})^2}$
> 
- expansion (Process of Normalize)
    
     $\left\|\zeta(t)^{\frac{1}{2}} p_n^{(t)} \chi^{(t)} \right\|^2_{\nu^{(t)}} = \int \left(\zeta(t)^{\frac{1}{2}} p_n^{(t)} \chi^{(t)} \right)^2 \frac{\omega^{(t)}}{\zeta(t)(\chi^{(t)})^2} = \int (p_n^{(t)})^2 \omega^{(t)} = \left\| p_n^{(t)} \right\|^2_{\mu^{(t)}} = 1$
    

> **$\nu^{(t)}$에 대한 orthogonal basis**
> 
> 
> ## **$\therefore\; g_n^{(t)} = \lambda_n \zeta(t)^{\frac{1}{2}}p_n^{(t)}\chi^{(t)}, \;\; n\in\mathbb{N}$**
> 
- scalar $\lambda_n$는 $g_n^{(t)}$를 scaling하기위해 추후 Legendre OPs를 scaling하는 과정에서 다룸
- 만약 $\lambda_n = \pm 1$ 인 경우,  basis $g_n^{(t)}$는 모든 t에 대해서 measure $\nu^{(t)}$에 대한 orthogonal basis이다.

> 최종적으로, 다음과 같은 꼴을 보인다. 
$\langle g_n^{(t)}, g_m^{(t)} \rangle_{\nu^{(t)}} = \lambda_n^2 \delta_{n,m}$
> 

## $\text{coeffient} \;c_n(t)$ Expansion

$g_n^{(t)}$는 이제 

1. normalized measure $\nu^{(t)}$에 대한 orthogonal basis
2. $\langle g_n^{(t)}, g_m^{(t)} \rangle_{\nu^{(t)}} = \lambda_n^2 \delta_{n,m}$로  Scaling factor $\lambda_n$(주어지는 measure와 OP의 조건에 따라 바뀜)를 무시하면 Kronecker delta에 따라서 1로 정규화된 정규 직교 기저

이제, 직관적으로 $f_{\leq t}$와 $g_n^{(t)}$의 Inner Product로 $f_{\leq t}$가 $g_n^{(t)}$에 Projection되었을 때의 coefficient $c_n(t)$를 구할 수 있다. 

![이해를 위해서 일반적인 Vector형태로 보여주는 예시](mamba_review_img/Untitled%2017.png)

이해를 위해서 일반적인 Vector형태로 보여주는 예시

전개 : 

 $\begin{aligned}c_n(t) &= \langle f_{\leq t}, g_n^{(t)}\rangle_{\nu^{(t)}}\\&= \displaystyle \int fg_n^{(t)}\dfrac{\omega^{(t)}}{\zeta(t)(\chi^{(t)})^2} \\&= \zeta(t)^{-\frac{1}{2}}\lambda_n \displaystyle\int fp_n^{(t)}\dfrac{\omega^{(t)}}{\chi^{(t)}} \end{aligned}$

> **HiPPO Framework의 기본 memory(:= $c_n(t)$ ) 식**
> 
> 
> ## $\therefore c_n(t) = \zeta(t)^{-\frac{1}{2}}\lambda_n \displaystyle\int fp_n^{(t)}\dfrac{\omega^{(t)}}{\chi^{(t)}}$
> 

---

# HiPPO-LegS : Scaled Measures for Timescale Robustness (이후에 논문들에 사용되는 HiPPO식)

![Untitled](mamba_review_img/Untitled%2018.png)

![Untitled](mamba_review_img/Untitled%2019.png)

- 함수를 입력받을 때, Sliding Window가 일반적이지만 Memory의 forgetting을 방지하기 위해 Window를 Scaling하는 방식으로 접근한 방식

Measure : 시간에 따라 변하는 weighting function, 계산에서 다양한 시간 간격의 중요성을 조절

> Densities $w(t,x) := \dfrac{\text{d}\mu^{(t)}}{\text{d}\lambda}(x)$
> 

> $\omega(t, x) = \dfrac{1}{t}\mathbb{I}_{[0, t]}$
> 

Basis : 주어진 함수 공간 내에서 orthogonality과 normalization를 유지하는 방식으로 다른 함수를 표현하는 데 사용되는 orthonormal basis functions 

> $g_n(t, x)=p_n(t,x) = (2n+1)^{\frac{1}{2}}P_n\bigg(\dfrac{2x}{t}-1\bigg)$
> 
- Step 1. Legendre Polynomials (about Orthogonal properties)
    
    이때의 $\omega^{\text{leg}} = \mathbf{1}_{[-1, 1]}$
    
    **서로 다른 $P_n(x)$들은 $-1 \leq x\leq1$에서 직교성을 가진다.**
    
    > $\displaystyle\int_{-1}^{1}P_n(x) P_m(x) dx = \dfrac{2}{2n+1}\delta_{nm}$
    > 
    > - $\delta_{nm} = \begin{cases} 1 \;\;\; n = m\\ 0 \;\;\; n \neq m\end{cases}$
    > - Scaling factor $\dfrac{2}{2n+1}$은 다음을 Legendre Polynomials 성질을 만족하기 위한 
    > $P_n(1) = 1$
    > $P_n(-1) = (-1)^n$
- Step 2. Shifted and Scaled Legendre polynomials
    
    $x \in [-1, 1] \rightarrow [0,t]$ 로 Shifting and Scaling하기 위해서는 $x \Rightarrow(=:) \dfrac{2x^\prime}{t}-1$ 으로 변환
    $dx = \dfrac{2}{t}dx^\prime$ (+ $\prime$은 임의로 넣음)
    
    그러면, 기존 식에서 $\dfrac{2n+1}{2}$를 양변에 곱해주면 우변에 $\delta_{nm}$만 남음
    
    좌변은 Lebesgue Integral 형식으로 바꿔주면,
    $\dfrac{2n+1}{2}\displaystyle\int P_n(x) P_m(x)\omega^{\text{leg}}(x) dx$
    
    $x = \dfrac{2x^\prime}{t}-1$ 을 넣어주면,
    
    $(2n+1)\displaystyle\int P_n\bigg(\dfrac{2x^\prime}{t}-1\bigg) P_m\bigg(\dfrac{2x^\prime}{t}-1\bigg)\omega^{\text{leg}}\bigg(\dfrac{2x^\prime}{t}-1\bigg) \dfrac{1}{t}dx^\prime$
    
    그러면 해당 식은 $[0, t]$에서 $\delta_{nm}$를 만족한다.
    
    $(2n+1)\displaystyle\int_0^t P_n\bigg(\dfrac{2x^\prime}{t}-1\bigg) P_m\bigg(\dfrac{2x^\prime}{t}-1\bigg)\omega^{\text{leg}}\bigg(\dfrac{2x^\prime}{t}-1\bigg) \dfrac{1}{t}dx^\prime =\delta_{nm}$
    
    이때, measure의 densitiy $\dfrac{1}{t}\mathbb{I}_{[0, t]}$ 로 바뀌고 
    위 식으로부터 nomalized OPs만 따로 쓰면($x^\prime$은 임의로 넣은 것이므로 $x$와의 기호상의 차이일 뿐, 동일하므료 $x^\prime$ ⇒ $x$),
    
    $(2n+1)^{1/2}P_n\bigg(\dfrac{2x}{t}-1\bigg)$
    

이후 단순 미분으로 State Space Model에 맞춰 전개하는 것이므로 단순하다. 

- State Space Model 수식
    - $x^\prime(t) = \mathbf{A}x(t) + \mathbf{B}u(t)$
    - $y(t) = \mathbf{C}x(t) + \mathbf{D}u(t)$
        - input $u(t) \in \mathbb{R}^{1\times D}$
        - state respresentation $x(t) \in \mathbb{R}^{N \times D}$
        - output $y(t) \in \mathbb{R}^{1\times D}$
    
    - 여기서 $\mathbf{D}$는 Deep Learning에서는 Skip connection이므로 생략이 가능하다.
        - 제어공학에서는 물리적으로 $\mathbf{D}$가 보통은 0이기 때문에 생략 가능
        - State Variable $x$의 정의에 의해 System이나 Response을 충분히 표현가능해야 하기 때문에 output $y$에 대해서 $\mathbf{C}x(t)$로 충분히 표현가능해야 하기 때문에 $\mathbf{D}$ 생략
    
    ![제어공학에서 다루는 SSM](mamba_review_img/Untitled%206.png)
    
    제어공학에서 다루는 SSM
    
    이후, continuous time에서 활용되는 SSM을 ML에서 사용하기 위해서 주로 사용되는 Discrete-time (language, DNA 등에 맞춰)으로 변환해야 한다. 
    

![Untitled](mamba_review_img/Untitled%2020.png)

위 식과 같이 전개하고 정리하면,

> $A_{nk} = \begin{cases} (2n+1)^{1/2}(2k+1)^{1/2} & \text{if} \; n > k \\ n+1 &\text{if}\; n=k \\ 0 & \text{if}\; n<k \end{cases}$
> 

> $B_{nk} = (2n+1)^{\frac{1}{2}}$
> 

State Space Model의 A, B Matrix를 구할 수 있다.

- $n$ : dimension
- $k$ : 각 n행의 index

# Discretization

[data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAABDUlEQVR4AYXRgUZDYRjH4TegFTKgpEqiFJgoWAoMEQGBgBboChaaAKxLKAhAhQqAdAmpBIQolkCFqp2nITvNKXuA7+/Hhzey5OWjE4Nq3rzY1f9/NGHPB549492+8Ww060iCS2XdctZdI3GsECmb+HJoIX6x6EgDm+lURTH+YB7V9nAqE5WNme4YKuOiY6iMe6PaQxUUIuTbswgFVNJwA8sO3Bn6yR6bWZMSNtJwDtuWfHpQxaPx9C9zadil7jrCigbq6UXceNIVKTWUIqypm2ytJdTiNyNeXclF6GttOVfeDEc7qzjR23r3OMFqZKng1kw0mXGLrfibHTScOZWgGv9TdC6ROFeMTgwYiIxvJzMRWQbeGZUAAAAASUVORK5CYII=](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAABDUlEQVR4AYXRgUZDYRjH4TegFTKgpEqiFJgoWAoMEQGBgBboChaaAKxLKAhAhQqAdAmpBIQolkCFqp2nITvNKXuA7+/Hhzey5OWjE4Nq3rzY1f9/NGHPB549492+8Ww060iCS2XdctZdI3GsECmb+HJoIX6x6EgDm+lURTH+YB7V9nAqE5WNme4YKuOiY6iMe6PaQxUUIuTbswgFVNJwA8sO3Bn6yR6bWZMSNtJwDtuWfHpQxaPx9C9zadil7jrCigbq6UXceNIVKTWUIqypm2ytJdTiNyNeXclF6GttOVfeDEc7qzjR23r3OMFqZKng1kw0mXGLrfibHTScOZWgGv9TdC6ROFeMTgwYiIxvJzMRWQbeGZUAAAAASUVORK5CYII=)

![Untitled](mamba_review_img/Untitled%2021.png)

- **generalized bilinear transform (GBT)**
    
    ![Untitled](mamba_review_img/Untitled%2022.png)
    
    - step size $\Delta t$가 주어졌을때,
    - forward Euler corresponds to α = 0, backward Euler to α = 1, and bilinear to α = 1/2

![Untitled](mamba_review_img/Untitled%2023.png)

# LSSL(The Linear State Space Layer)

![Untitled](mamba_review_img/Untitled%2024.png)

### #1. Continuous-time

- Spectrum of Sequence Data
    
    ![Untitled](mamba_review_img/Untitled%2025.png)
    

![Untitled](mamba_review_img/Untitled%2026.png)

- Neural ODEs에서 나온 적절한 Step size $\Delta t$ 를 ODE solver로 구하는 것을 적용했다. (처음에는, naive LSSL)
- 하지만, ODE solver로 구하는 것보다 다양한 초기 값을 $\Delta t$에 준 후, $\Delta t$에 대해서 학습가능한 파라미터로 적용하는 것이 더 효과적인 것이 실험적으로 검증했다. (+ $\Delta t$ 를 다양하게 표현한다는 가정하에, 기존 RNNs의 성능을 보여준다는 이론적 검증에 활용도 함) (⇒ LSSL)
- $[\Delta t_{\text{min}},  \Delta t_{\text{max}}]$에 대해서 Log-Uniform으로 초깃값 부여
    - $\Delta t_{\text{min}}$과 $\Delta t_{\text{max}}$ 는 100배 차이로 설정
- dependency $\propto \dfrac{1}{\Delta t}$

⇒ 다양한 Spectrum의 Step size $\Delta t$를 사용하니 다양한 Sequence Data별 최적의 step size를 찾아서 사용할 수 있다.

- Discrete Method는 GBT

![Untitled](mamba_review_img/Untitled%2027.png)

![Matrix A로 드는 예시](mamba_review_img/Untitled%2028.png)

Matrix A로 드는 예시

$u \in \mathbb{R}^{L \times H}$

$x_t \in \mathbb{R}^{H \times N}$

$y \in \mathbb{R}^{H \times L}$

Each feature $h ∈ [H]$ defines a sequence$(u_t^{(h)})_{t∈[L]}$

### #2. Recurrent

![Untitled](mamba_review_img/Untitled%2029.png)

이산화된 SSM은 Linear RNN으로 볼 수 있다.

1. Recurrent 성질
2. GRU등 RNNs 수식들과 유사

이런 Recurrent한 성질은 End-to-End 구조가 가능하며, 

기존에 계산된 값($x(t)$)만 가져오면 이전 정보들이 전부 근사화되었으니 Transformer처럼 다시 Sequence Data의 처음부터 계산할 필요없이 빠른 추론이 가능하다.

![D는 생략, $x_{-1} = 0$](mamba_review_img/Untitled%2030.png)

D는 생략, $x_{-1} = 0$

![Untitled](mamba_review_img/Untitled%2022.png)

IF. $\alpha = A = B = 1, \; N = 1$

![Untitled](mamba_review_img/Untitled%2031.png)

### #3. Convolutional

![Untitled](mamba_review_img/Untitled%2032.png)

![Untitled](mamba_review_img/Untitled%2033.png)

![Untitled](mamba_review_img/Untitled%2034.png)

![kernel3.gif](mamba_review_img/kernel3.gif)

$C\bar{B}$에 $\bar{A}$가 $L-1$ 번 곱해지는 Kernel을 구하고 이를 input $u$와 Convolution 연산을 할 수 있게된다. 이는 병렬 연산으로 더 빠른 Traning이 가능해진다. (Convolution 연산은 FFT로 가속화)

### LSSL Problem.

빠른 Traning을 위해서 $\bar{A}$를 계속 곱하면  Vanishing Gradient Problem이 발생한다. 특히, 기존의 HiPPO Matrix A와 같은 고정된 값의 지속적인 행렬연산은 더욱 Vanishing Gradient Problem이 심하다.

(+ sparse matrix 방지를 위한 자동 알고리즘 $\mathcal{K}_L := \text{Krylov fuction}$도 계산된 값이 Cache 형태로 저장되어야하는 등 bottleneck이 발생)

따라서, LSSL에서는 $\Delta t$와 더불어 $A$ 를 Random 초기화하고 Tranable Parameter로 바꾸었다.

하지만, $A$도 Trainable하게 바꾼 것은 Gradient Vanishing Problem을 충분히 방지하거나 Bottleneck을 해결하지 못하기 때문에 임시방편이다. 

**⇒ 기존의 이론적 보장이 되어있는 HiPPO로부터 구해진 Matrix A를 재구조화해서 이론적 보장이 된 Long Sequence Data의 처리와 계산, Memory 효율성을 높임** 

**(S4 Model, Efficiently Modeling Long Sequences with Structured State Spaces)**

# S4 Model

이전 LSSL 논문에서 Matrix A가 $\text{3-}quasiseparable$ 하다고 하고 있다. 

$Definition$

행렬 $M \in \mathbb{C}^{N \times N}$가 $(r_L, r_U)\text{-}quasiseparable$ 이라는 것은 아래와 같다 :

- $r_L = \max_{1 \leq k \leq N-1} \text{rank} \, M(k+1:N, 1:k)$
- $r_U = \max_{1 \leq k \leq N-1} \text{rank} \, M(1:k, k+1:N)$

⇒ 하삼각행렬에서 어떤 하위 행렬을 뽑아도 Rank $r_L$을 못 넘긴다.

⇒ 상삼각행렬에서 어떤 하위 행렬을 뽑아도 Rank $r_U$을 못 넘긴다.

- Off-Diagonal Blocks은 모두 Low-Rank이다.

![Untitled](mamba_review_img/Untitled%2035.png)

따라서, 충분히 Low-Rank로 분해할 수 있는 확장가능성이 있기때문에 이를 알아보려했다.

![Untitled](mamba_review_img/Untitled%2036.png)

![Untitled](mamba_review_img/Untitled%2037.png)

![Untitled](mamba_review_img/Untitled%2038.png)

### Try 1. Diagonalization

⇒ **X** (numerical issues)

### Try 2. Normal Plus Low-Rank

HiPPO Matrix ⇒ Normal Matrix & Low-Rank Matrix로 분해

![Untitled](mamba_review_img/Untitled%2039.png)

![Untitled](mamba_review_img/Untitled%2040.png)

$A = \Lambda - PQ^*$ : Woodbury Identity

이후, 

![Untitled](mamba_review_img/Untitled%2041.png)

이런식으로 변환하면 대각 행렬 요소만 따로 뺄 수 있기 때문에 Diagonal Plus Low-Rank (DPLR)로 변환이 가능하다.

![Untitled](mamba_review_img/Untitled%2042.png)

$\bar{A}$ 중 ($I+\Delta/2\cdot A$)만 $A = \Lambda - PQ^*$를 대입 후 정리

![Untitled](mamba_review_img/Untitled%2043.png)

$(I+\Delta/2\cdot A)^{-1}$ 도 같은  식으로 진행

![Untitled](mamba_review_img/Untitled%2044.png)

![Untitled](mamba_review_img/Untitled%2045.png)

Woodbury’s Identity의 역행렬

[Woodbury matrix identity](https://en.wikipedia.org/wiki/Woodbury_matrix_identity)

최종적인 Discrete SSM

![Untitled](mamba_review_img/Untitled%2046.png)

![Untitled](mamba_review_img/Untitled%2047.png)

![Untitled](mamba_review_img/Untitled%2048.png)

---

# Mamba: Linear-Time Sequence Modeling with Selective State Spaces (S6 Model)

SSMs는 Long Sequence Modeling이 고려되어 만들어졌지만 Language 같은 Main Modality에서는 Attention Mechanism만한 성능 X

![Untitled](mamba_review_img/Untitled%2049.png)

기존 S4 Model(Mamba 논문에서 SSM은 모두 S4 Model을 지칭)에서 바뀐점

### Discretization

 Discretization : GBT → ZOH(Zero-Order Hold)

![Untitled](mamba_review_img/Untitled%2050.png)

![Untitled](mamba_review_img/Untitled%2051.png)

한 주기 동안 현재값이 유지되는 Discretization

### Linear Time Invariance (LTI)

- 기존에 Convolution 연산과 Recurrent 연산을 동등하게 본 건 이 LTI System이라는 전제 (Informel)

⇒ 하지만, LTI System이 특정 Domain Data를 Modeling하는데 한계가 있음을 분석함

### Selective State Space Models

![Untitled](mamba_review_img/Untitled%2052.png)

Transformer : No Compressed (Not Efficiency) but Effectiveness

$\text{Effectiveness}  \propto \dfrac{1}{\text{Efficiency(Compressed)}}$

![Untitled](mamba_review_img/Untitled%2053.png)

### Problem (intuition)

![Untitled](mamba_review_img/Untitled%2054.png)

1. Selective Copying Task
    
    : Input의 일부를 Copying 해서 순서대로 출력
    
    ![Untitled](mamba_review_img/Untitled%2055.png)
    
2. Induction Heads Task (related In-Context Learning)
    
    : Input에서의 발견된 Pattern을 재현
    
    ![Untitled](mamba_review_img/Untitled%2056.png)
    

### Solution

![Untitled](mamba_review_img/Untitled%2057.png)

1. Selective Copying을 하기 위해서는 선택적으로 정보를 택하고 무시하는 과정이 필요하다. 
    
    ⇒ LTI System은 모든 context에 대해서 효과적으로 무시 **X**
    
    이를 Gated MLP로 구현
    
    ![Untitled](mamba_review_img/Untitled%2058.png)
    
    Variable length (Step size) $\Delta$에 대해 의존적으로 만듦
    
    $\Delta$ → $\infin$ : 현재 Input에 집중
    
    $\Delta$ → $0$ : 현재 Input 무시
    
    ![Untitled](mamba_review_img/Untitled%2059.png)
    
    $\Delta$ 만큼 Broadcast/Repeating 
    

H3(Hungry Hungry HiPPO) : SSM을 NLP Domain에 본격적으로 적용함

1. Linear Attention (Softmax 근사화)
2. FlashConv (Hardware Modeling)

![Untitled](mamba_review_img/Untitled%2060.png)

![Untitled](mamba_review_img/Untitled%2061.png)

![Untitled](mamba_review_img/Untitled%2062.png)

![Untitled](mamba_review_img/Untitled%2063.png)

![Untitled](mamba_review_img/Untitled%2064.png)

![Untitled](mamba_review_img/Untitled%2065.png)

![Untitled](mamba_review_img/Untitled%2066.png)

# # 참고.

---

### #. Hilbert Space

- “Hilbert space” is often reserved for an infinite-dimensional inner product space having the property that it is complete or closed.
- 유한 차원인 Euclidean space($\mathbb{R}^n$)
    
    ⇒ **무한 차원으로 일반화(극한)**
    
- Function의 $[0, \infin)$ 범위에서의 내적을 고려해야하기 때문에 채택

![Untitled](mamba_review_img/Untitled%2067.png)

### #. Approximations of differential equations.

SSM에서 $\dot{x}(t) = \mathbf{A}x(t) + \mathbf{B}u(t)$ 를 구하는 것을 Discrete한 컴퓨터가 연산하기 위해서 Numerical하게 풀어야한다. (Numerical ODE)

$\dot{x}(t) = \mathbf{A}x(t) + \mathbf{B}u(t)$ 이 미분방정식을 풀기 위해서 ODE Solver를 적용해 미분 방정식(Difference Equation)을 차분 방정식(Difference Equation)으로 변환하고 다음 Time Step의 $x(t)$를 구한다.

더 자세하게 알아보면 아래와 같다.

$f$ : Network

$t$ : time

$x(t)$ : Hidden State (or Memory Cell)

일 때, $\mathbf{A}x(t) + \mathbf{B}u(t)$를 Network $f$로 두면 미분 방정식을 Integral Solution으로 표현 할 수 있다.

$\text{Any differential equation}  \\ \dot{x}(t) = f(t, x(t))$

$\Leftrightarrow$

$\text{Integral Solution} \\   x(t) = x(t_0) + \displaystyle \int_{t_0}^t f(s, x(s))ds$

> Picard iteration (함수의 재귀 공식화)
> 
> 
> $x_{i+1}(t) := x_0(t) + \displaystyle \int_{t_0}^t f(s, x_i(s))ds$
> 

$x_0(t), x_1(t), \cdots$ , 각 $x_n(t)$가 Integral Solution $x(t)$에 근사된다고 가정하면, 아래와 같이 시점 $t$에 대해서 Hidden State의 현재 시점, 다음 시점으로 표현할 수 있다.

$x(t_{i+1}) = x(t_i) + \displaystyle\int_{t_i}^{t_{i+1}}f(s, x(s))ds$ 혹은 $x(t_{k}) = x(t_{k-1}) + \displaystyle\int_{t_{k-1}}^{t_{k}}f(s, x(s))ds$

![Untitled](mamba_review_img/Untitled%2068.png)

---

---

![Untitled](mamba_review_img/Untitled%2069.png)

![Untitled](mamba_review_img/Untitled%2070.png)

$s_B(x) = \text{Linear}_N(x)$

$s_C(x) = \text{Linear}_N(x)$

$s_\Delta(x) = \text{Broadcast}_D(\text{Linear}_1(x))$

$\tau_\Delta = \text{softplus} = \dfrac{1}{\beta}*\log (1+\exp(\beta*x))$

[Papers with Code - Softplus Explained](https://paperswithcode.com/method/softplus)

![Untitled](mamba_review_img/Untitled%2071.png)
`;

            // Convert Markdown to HTML using marked function
            const htmlContent = marked.parse(markdownContent);

            // Insert the converted HTML into the page
            document.getElementById("markdown-content").innerHTML = htmlContent;

            MathJax.typeset();
        });
    </script>
</body>

</html>
